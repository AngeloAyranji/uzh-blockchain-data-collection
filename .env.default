# General
PROJECT_NAME="bdc"
DATA_DIR="./data"
LOG_LEVEL="INFO"

# The owner of the data directory, if left blank the current user id that started this script will be used
DATA_UID=
# group id (e.g of "bdlt" group - default `getent group bdlt | cut -d: -f3`)
DATA_GID=

# Amount of consumer containers
N_CONSUMERS=2
# Amount of consumer instances of DataConsumer class within a single consumer container
# (this approach leverages multiprocessing along with asyncio so the total amount
# of Kafka consumers equals N_CONSUMERS * N_CONSUMER_INSTANCES)
N_CONSUMER_INSTANCES=2
# Kafka
# number of partitions for each topic (blockchain), must be larger than the
# total amount of Kafka consumers (N_CONSUMERS * N_CONSUMER_INSTANCES), ideally 1-2x
KAFKA_N_PARTITIONS=$((2 * $N_CONSUMER_INSTANCES * $N_CONSUMERS))

# Sentry
# Leave empty if not needed
SENTRY_DSN=

# PostgreSQL
# port to publish on the host (abacus-3)
POSTGRES_HOST_PORT=13338
# port to use for postgres on the docker network
POSTGRES_CONTAINER_PORT=5432
# port to use for pgbouncer / db_pool on the docker network
POSTGRES_POOL_PORT=6432
POSTGRES_USER="user"
POSTGRES_PASSWORD="postgres"
POSTGRES_DB="db"
# Should be slighly larger than the total # of consumers
POSTGRES_MAX_CONNECTIONS=500
# Maximum amount of connections to the pool
POSTGRES_POOL_SIZE=1000

# Erigon (ETH node) connectivity
ERIGON_PORT=8547
ERIGON_HOST="host.docker.internal"

# Timeouts
WEB3_REQUESTS_TIMEOUT=30            # timeout for every request (in seconds)
WEB3_REQUESTS_RETRY_LIMIT=10        # maximum amount of retries for each request
WEB3_REQUESTS_RETRY_DELAY=5         # delay between retries (in seconds)
KAFKA_EVENT_RETRIEVAL_TIMEOUT=600   # timeout for retrieving events from Kafka (in seconds)
